{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0a8ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import association_rules, fpgrowth\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "from fswe_demo.infra.db.get_conn import get_db_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70cde91",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = get_db_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d045a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from the db\n",
    "\n",
    "transaction_df = pd.read_sql_table(\"int_product_baskets\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b6181",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfafb44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic EDA\n",
    "transaction_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b911faa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_baskets_list = transaction_df[\"product_basket\"].tolist()\n",
    "product_baskets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97feca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the transactions\n",
    "te = TransactionEncoder()\n",
    "encoded_transaction_df = pd.DataFrame(\n",
    "    te.fit(product_baskets_list).transform(product_baskets_list), columns=te.columns_,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7f5881",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_transaction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c946940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = encoded_transaction_df\n",
    "\n",
    "print(df.shape)  # (n_transactions, n_items)\n",
    "print(df.dtypes.unique())  # should be only bool (or 0/1 numeric)\n",
    "print(df.sum().sum())  # total True/1s; should be > 0\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab70a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tx = len(df)\n",
    "per_item_support = df.mean().sort_values(ascending=False)  # support = fraction of txns\n",
    "print(per_item_support.head(10))\n",
    "print(\"Support threshold count =\", 0.2 * n_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa812b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Parameter Tweaking\n",
    "\n",
    "# Test different support thresholds to maximize item pairs while maintaining quality\n",
    "support_candidates = [0.0005, 0.001, 0.002, 0.005, 0.01]\n",
    "optimization_results = []\n",
    "\n",
    "for min_support in support_candidates:\n",
    "    print(f\"Testing support threshold: {min_support:.4f}\")\n",
    "\n",
    "    # Generate frequent itemsets\n",
    "    freq_itemsets = fpgrowth(\n",
    "        encoded_transaction_df, min_support=min_support, use_colnames=True,\n",
    "    )\n",
    "\n",
    "    if len(freq_itemsets) > 0:\n",
    "        # Add length column\n",
    "        freq_itemsets[\"length\"] = freq_itemsets[\"itemsets\"].apply(lambda x: len(x))\n",
    "\n",
    "        # Count itemsets by length\n",
    "        length_counts = freq_itemsets[\"length\"].value_counts().sort_index()\n",
    "\n",
    "        # Try to generate association rules\n",
    "        itemset_pairs = freq_itemsets[freq_itemsets[\"length\"] == 2]\n",
    "        rules_count = 0\n",
    "\n",
    "        if len(itemset_pairs) > 0:\n",
    "            try:\n",
    "                rules = association_rules(\n",
    "                    freq_itemsets, metric=\"confidence\", min_threshold=0.1,\n",
    "                )\n",
    "                rules_count = len(rules)\n",
    "            except:\n",
    "                rules_count = 0\n",
    "\n",
    "        optimization_results.append(\n",
    "            {\n",
    "                \"min_support\": min_support,\n",
    "                \"total_itemsets\": len(freq_itemsets),\n",
    "                \"single_items\": length_counts.get(1, 0),\n",
    "                \"item_pairs\": length_counts.get(2, 0),\n",
    "                \"larger_sets\": sum(length_counts.get(i, 0) for i in range(3, 20)),\n",
    "                \"association_rules\": rules_count,\n",
    "                \"coverage_ratio\": length_counts.get(2, 0)\n",
    "                / max(length_counts.get(1, 1), 1),  # pairs per single item\n",
    "            },\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"{len(freq_itemsets)} itemsets, {length_counts.get(2, 0)} pairs, {rules_count} rules\",\n",
    "        )\n",
    "    else:\n",
    "        print(\"No frequent itemsets found\")\n",
    "        optimization_results.append(\n",
    "            {\n",
    "                \"min_support\": min_support,\n",
    "                \"total_itemsets\": 0,\n",
    "                \"single_items\": 0,\n",
    "                \"item_pairs\": 0,\n",
    "                \"larger_sets\": 0,\n",
    "                \"association_rules\": 0,\n",
    "                \"coverage_ratio\": 0,\n",
    "            },\n",
    "        )\n",
    "\n",
    "# Display optimization results\n",
    "opt_df = pd.DataFrame(optimization_results)\n",
    "print(opt_df.to_string(index=False))\n",
    "\n",
    "# Select optimal parameters (maximize item pairs while having reasonable rules)\n",
    "optimal_row = (\n",
    "    opt_df[opt_df[\"item_pairs\"] > 0].iloc[0]\n",
    "    if len(opt_df[opt_df[\"item_pairs\"] > 0]) > 0\n",
    "    else opt_df.iloc[0]\n",
    ")\n",
    "optimal_support = optimal_row[\"min_support\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b88c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate Comprehensive Itemsets and Rules for Recommendations\n",
    "# Use optimal support threshold\n",
    "print(f\"Using optimal support threshold: {optimal_support:.4f}\")\n",
    "\n",
    "# Generate frequent itemsets\n",
    "frequent_itemsets = fpgrowth(\n",
    "    encoded_transaction_df, min_support=optimal_support, use_colnames=True,\n",
    ")\n",
    "frequent_itemsets[\"length\"] = frequent_itemsets[\"itemsets\"].apply(lambda x: len(x))\n",
    "\n",
    "print(f\"Generated {len(frequent_itemsets)} frequent itemsets\")\n",
    "\n",
    "# Analyze itemset distribution\n",
    "length_distribution = frequent_itemsets[\"length\"].value_counts().sort_index()\n",
    "print(\"Itemset distribution:\")\n",
    "for length, count in length_distribution.items():\n",
    "    print(f\"   Length {length}: {count} itemsets\")\n",
    "\n",
    "# Generate association rules with multiple confidence thresholds\n",
    "confidence_thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "best_rules = None\n",
    "best_threshold = None\n",
    "\n",
    "print(\"Finding optimal confidence threshold:\")\n",
    "for conf_threshold in confidence_thresholds:\n",
    "    try:\n",
    "        rules = association_rules(\n",
    "            frequent_itemsets, metric=\"confidence\", min_threshold=conf_threshold,\n",
    "        )\n",
    "        if len(rules) > 0:\n",
    "            print(\n",
    "                f\"   Confidence {conf_threshold:.1f}: {len(rules)} rules (avg lift: {rules['lift'].mean():.2f})\",\n",
    "            )\n",
    "            if best_rules is None or len(rules) > len(best_rules):\n",
    "                best_rules = rules\n",
    "                best_threshold = conf_threshold\n",
    "    except:\n",
    "        print(f\"   Confidence {conf_threshold:.1f}: No rules generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c1cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Build Item-to-Item Recommendation Matrix\n",
    "\n",
    "\n",
    "def build_item_recommendation_matrix(rules_df, itemsets_df, top_k=10):\n",
    "    # Get all unique items from frequent itemsets\n",
    "    all_items = set()\n",
    "    for itemset in itemsets_df[\"itemsets\"]:\n",
    "        all_items.update(itemset)\n",
    "\n",
    "    all_items = sorted(list(all_items))\n",
    "    print(f\"Total unique items in catalog: {len(all_items)}\")\n",
    "\n",
    "    # Initialize recommendation matrix\n",
    "    recommendations = {}\n",
    "\n",
    "    # Method 1: Use association rules (antecedent -> consequent)\n",
    "    if len(rules_df) > 0:\n",
    "        for _, rule in rules_df.iterrows():\n",
    "            antecedents = list(rule[\"antecedents\"])\n",
    "            consequents = list(rule[\"consequents\"])\n",
    "\n",
    "            # Add recommendations for each antecedent\n",
    "            for ant_item in antecedents:\n",
    "                if ant_item not in recommendations:\n",
    "                    recommendations[ant_item] = []\n",
    "\n",
    "                for cons_item in consequents:\n",
    "                    recommendations[ant_item].append(\n",
    "                        {\n",
    "                            \"item\": cons_item,\n",
    "                            \"confidence\": rule[\"confidence\"],\n",
    "                            \"lift\": rule[\"lift\"],\n",
    "                            \"support\": rule[\"support\"],\n",
    "                            \"method\": \"association_rule\",\n",
    "                        },\n",
    "                    )\n",
    "\n",
    "    # Method 2: Use frequent item pairs (co-occurrence based)\n",
    "    item_pairs = itemsets_df[itemsets_df[\"length\"] == 2]\n",
    "    print(f\"Processing {len(item_pairs)} frequent item pairs\")\n",
    "\n",
    "    for _, pair_row in item_pairs.iterrows():\n",
    "        items_in_pair = list(pair_row[\"itemsets\"])\n",
    "        if len(items_in_pair) == 2:\n",
    "            item1, item2 = items_in_pair\n",
    "            pair_support = pair_row[\"support\"]\n",
    "\n",
    "            # Calculate individual item supports\n",
    "            item1_support = encoded_transaction_df[item1].mean()\n",
    "            item2_support = encoded_transaction_df[item2].mean()\n",
    "\n",
    "            # Calculate confidence and lift for both directions\n",
    "            confidence_1_to_2 = pair_support / item1_support if item1_support > 0 else 0\n",
    "            confidence_2_to_1 = pair_support / item2_support if item2_support > 0 else 0\n",
    "\n",
    "            lift_1_to_2 = confidence_1_to_2 / item2_support if item2_support > 0 else 0\n",
    "            lift_2_to_1 = confidence_2_to_1 / item1_support if item1_support > 0 else 0\n",
    "\n",
    "            # Add bidirectional recommendations\n",
    "            for source_item, target_item, conf, lift_val in [\n",
    "                (item1, item2, confidence_1_to_2, lift_1_to_2),\n",
    "                (item2, item1, confidence_2_to_1, lift_2_to_1),\n",
    "            ]:\n",
    "                if source_item not in recommendations:\n",
    "                    recommendations[source_item] = []\n",
    "\n",
    "                recommendations[source_item].append(\n",
    "                    {\n",
    "                        \"item\": target_item,\n",
    "                        \"confidence\": conf,\n",
    "                        \"lift\": lift_val,\n",
    "                        \"support\": pair_support,\n",
    "                        \"method\": \"frequent_pair\",\n",
    "                    },\n",
    "                )\n",
    "\n",
    "    # Sort and limit recommendations for each item\n",
    "    final_recommendations = {}\n",
    "    items_with_recommendations = 0\n",
    "\n",
    "    for item, recs in recommendations.items():\n",
    "        # Remove duplicates and sort by lift then confidence\n",
    "        unique_recs = {}\n",
    "        for rec in recs:\n",
    "            target_item = rec[\"item\"]\n",
    "            if (\n",
    "                target_item not in unique_recs\n",
    "                or rec[\"lift\"] > unique_recs[target_item][\"lift\"]\n",
    "            ):\n",
    "                unique_recs[target_item] = rec\n",
    "\n",
    "        # Sort by lift (descending) then confidence (descending)\n",
    "        sorted_recs = sorted(\n",
    "            unique_recs.values(),\n",
    "            key=lambda x: (x[\"lift\"], x[\"confidence\"]),\n",
    "            reverse=True,\n",
    "        )\n",
    "\n",
    "        # Keep top K recommendations\n",
    "        final_recommendations[item] = sorted_recs[:top_k]\n",
    "\n",
    "        if len(sorted_recs) > 0:\n",
    "            items_with_recommendations += 1\n",
    "\n",
    "    return final_recommendations, all_items\n",
    "\n",
    "\n",
    "# Build the recommendation matrix\n",
    "item_recommendations, catalog_items = build_item_recommendation_matrix(\n",
    "    best_rules if best_rules is not None else pd.DataFrame(),\n",
    "    frequent_itemsets,\n",
    "    top_k=10,\n",
    ")\n",
    "\n",
    "print(f\"Items with recommendations: {len(item_recommendations)}\")\n",
    "total_recommendations = sum(len(recs) for recs in item_recommendations.values())\n",
    "print(f\"Total recommendation pairs: {total_recommendations}\")\n",
    "if len(item_recommendations) > 0:\n",
    "    avg_recs_per_item = total_recommendations / len(item_recommendations)\n",
    "    print(f\"Average recommendations per item: {avg_recs_per_item:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b7a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemRecommendationEngine:\n",
    "    def __init__(self, recommendation_matrix, fallback_items=None):\n",
    "        self.recommendation_matrix = recommendation_matrix\n",
    "        self.fallback_items = fallback_items or []\n",
    "        self.total_items = len(recommendation_matrix)\n",
    "\n",
    "    def get_recommendations(\n",
    "        self, item_id, num_recommendations=5, min_lift=1.0, min_confidence=0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Get recommendations for a specific item\n",
    "\n",
    "        Args:\n",
    "            item_id: Product ID to get recommendations for\n",
    "            num_recommendations: Number of recommendations to return\n",
    "            min_lift: Minimum lift threshold for recommendations\n",
    "            min_confidence: Minimum confidence threshold\n",
    "\n",
    "        Returns:\n",
    "            List of recommended items with scores\n",
    "\n",
    "        \"\"\"\n",
    "        if item_id not in self.recommendation_matrix:\n",
    "            return {\n",
    "                \"item_id\": item_id,\n",
    "                \"recommendations\": self.fallback_items[:num_recommendations],\n",
    "                \"method\": \"fallback\",\n",
    "                \"message\": \"No specific recommendations found, using popular items\",\n",
    "            }\n",
    "\n",
    "        # Filter recommendations by quality thresholds\n",
    "        candidates = self.recommendation_matrix[item_id]\n",
    "        filtered_recs = [\n",
    "            rec\n",
    "            for rec in candidates\n",
    "            if rec[\"lift\"] >= min_lift and rec[\"confidence\"] >= min_confidence\n",
    "        ]\n",
    "\n",
    "        # Limit to requested number\n",
    "        final_recs = filtered_recs[:num_recommendations]\n",
    "\n",
    "        return {\n",
    "            \"item_id\": item_id,\n",
    "            \"recommendations\": final_recs,\n",
    "            \"method\": \"fpgrowth\",\n",
    "            \"total_candidates\": len(candidates),\n",
    "            \"after_filtering\": len(filtered_recs),\n",
    "            \"returned\": len(final_recs),\n",
    "        }\n",
    "\n",
    "    def get_batch_recommendations(self, item_ids, num_recommendations=5):\n",
    "        \"\"\"Get recommendations for multiple items at once\"\"\"\n",
    "        return {\n",
    "            item_id: self.get_recommendations(item_id, num_recommendations)\n",
    "            for item_id in item_ids\n",
    "        }\n",
    "\n",
    "    def get_similar_items(self, item_id, similarity_threshold=2.0):\n",
    "        \"\"\"Get items similar to given item (high lift values)\"\"\"\n",
    "        if item_id not in self.recommendation_matrix:\n",
    "            return []\n",
    "\n",
    "        similar_items = [\n",
    "            rec\n",
    "            for rec in self.recommendation_matrix[item_id]\n",
    "            if rec[\"lift\"] >= similarity_threshold\n",
    "        ]\n",
    "\n",
    "        return sorted(similar_items, key=lambda x: x[\"lift\"], reverse=True)\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\"Get engine statistics\"\"\"\n",
    "        if not self.recommendation_matrix:\n",
    "            return {\"total_items\": 0, \"coverage\": 0}\n",
    "\n",
    "        total_recs = sum(len(recs) for recs in self.recommendation_matrix.values())\n",
    "        avg_recs = (\n",
    "            total_recs / len(self.recommendation_matrix)\n",
    "            if self.recommendation_matrix\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"total_items_with_recs\": len(self.recommendation_matrix),\n",
    "            \"total_recommendation_pairs\": total_recs,\n",
    "            \"average_recs_per_item\": round(avg_recs, 2),\n",
    "            \"coverage_percentage\": round(\n",
    "                len(self.recommendation_matrix) / max(self.total_items, 1) * 100, 1,\n",
    "            ),\n",
    "        }\n",
    "\n",
    "\n",
    "# Create fallback recommendations (most popular items)\n",
    "popular_items = (\n",
    "    encoded_transaction_df.sum().sort_values(ascending=False).head(20).index.tolist()\n",
    ")\n",
    "\n",
    "# Initialize the recommendation engine\n",
    "rec_engine = ItemRecommendationEngine(\n",
    "    recommendation_matrix=item_recommendations, fallback_items=popular_items,\n",
    ")\n",
    "\n",
    "stats = rec_engine.get_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "# Test the recommendation engine\n",
    "test_items = list(item_recommendations.keys())[:3] if item_recommendations else []\n",
    "\n",
    "for test_item in test_items:\n",
    "    print(f\"\\nRecommendations for {test_item}\")\n",
    "    result = rec_engine.get_recommendations(test_item, num_recommendations=5)\n",
    "\n",
    "    print(f\"Method: {result['method']}\")\n",
    "    print(f\"Candidates: {result.get('total_candidates', 0)}\")\n",
    "\n",
    "    for i, rec in enumerate(result[\"recommendations\"], 1):\n",
    "        if isinstance(rec, dict):\n",
    "            print(f\"{i}. {rec['item']}\")\n",
    "            print(f\"Confidence: {rec['confidence']:.3f}, Lift: {rec['lift']:.2f}\")\n",
    "        else:\n",
    "            print(f\"{i}. {rec} (fallback)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74929e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Recommendation Analysis and Visualization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Analyze recommendation quality distribution\n",
    "def analyze_recommendation_quality(rec_matrix):\n",
    "    \"\"\"Analyze the quality distribution of recommendations\"\"\"\n",
    "    all_confidences = []\n",
    "    all_lifts = []\n",
    "    all_supports = []\n",
    "\n",
    "    for item, recs in rec_matrix.items():\n",
    "        for rec in recs:\n",
    "            all_confidences.append(rec[\"confidence\"])\n",
    "            all_lifts.append(rec[\"lift\"])\n",
    "            all_supports.append(rec[\"support\"])\n",
    "\n",
    "    return {\n",
    "        \"confidences\": all_confidences,\n",
    "        \"lifts\": all_lifts,\n",
    "        \"supports\": all_supports,\n",
    "        \"total_pairs\": len(all_confidences),\n",
    "    }\n",
    "\n",
    "\n",
    "quality_metrics = analyze_recommendation_quality(item_recommendations)\n",
    "\n",
    "print(f\"Total recommendation pairs: {quality_metrics['total_pairs']}\")\n",
    "if quality_metrics[\"total_pairs\"] > 0:\n",
    "    print(f\"Average confidence: {np.mean(quality_metrics['confidences']):.3f}\")\n",
    "    print(f\"Average lift: {np.mean(quality_metrics['lifts']):.3f}\")\n",
    "    print(f\"Average support: {np.mean(quality_metrics['supports']):.4f}\")\n",
    "    print(\n",
    "        f\"High-quality pairs (lift > 2): {sum(1 for l in quality_metrics['lifts'] if l > 2)}\",\n",
    "    )\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "if quality_metrics[\"total_pairs\"] > 0:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(\n",
    "        \"FP Growth Item-to-Item Recommendation Analysis\", fontsize=16, fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    # 1. Confidence distribution\n",
    "    axes[0, 0].hist(\n",
    "        quality_metrics[\"confidences\"], bins=20, alpha=0.7, edgecolor=\"black\",\n",
    "    )\n",
    "    axes[0, 0].set_xlabel(\"Confidence\")\n",
    "    axes[0, 0].set_ylabel(\"Frequency\")\n",
    "    axes[0, 0].set_title(\"Distribution of Recommendation\\\\nConfidence Values\")\n",
    "    axes[0, 0].axvline(\n",
    "        np.mean(quality_metrics[\"confidences\"]),\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"Mean\",\n",
    "    )\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "    # 2. Lift distribution\n",
    "    axes[0, 1].hist(quality_metrics[\"lifts\"], bins=20, alpha=0.7, edgecolor=\"black\")\n",
    "    axes[0, 1].set_xlabel(\"Lift\")\n",
    "    axes[0, 1].set_ylabel(\"Frequency\")\n",
    "    axes[0, 1].set_title(\"Distribution of Recommendation\\\\nLift Values\")\n",
    "    axes[0, 1].axvline(1, color=\"red\", linestyle=\"--\", label=\"Lift = 1\")\n",
    "    axes[0, 1].axvline(\n",
    "        np.mean(quality_metrics[\"lifts\"]), color=\"orange\", linestyle=\"--\", label=\"Mean\",\n",
    "    )\n",
    "    axes[0, 1].legend()\n",
    "\n",
    "    # 3. Support distribution\n",
    "    axes[0, 2].hist(quality_metrics[\"supports\"], bins=20, alpha=0.7, edgecolor=\"black\")\n",
    "    axes[0, 2].set_xlabel(\"Support\")\n",
    "    axes[0, 2].set_ylabel(\"Frequency\")\n",
    "    axes[0, 2].set_title(\"Distribution of Recommendation\\\\nSupport Values\")\n",
    "    axes[0, 2].axvline(\n",
    "        np.mean(quality_metrics[\"supports\"]), color=\"red\", linestyle=\"--\", label=\"Mean\",\n",
    "    )\n",
    "    axes[0, 2].legend()\n",
    "\n",
    "    # 4. Confidence vs Lift scatter\n",
    "    axes[1, 0].scatter(\n",
    "        quality_metrics[\"confidences\"],\n",
    "        quality_metrics[\"lifts\"],\n",
    "        c=quality_metrics[\"supports\"],\n",
    "        alpha=0.6,\n",
    "        cmap=\"viridis\",\n",
    "    )\n",
    "    axes[1, 0].set_xlabel(\"Confidence\")\n",
    "    axes[1, 0].set_ylabel(\"Lift\")\n",
    "    axes[1, 0].set_title(\"Confidence vs Lift\\\\n(colored by Support)\")\n",
    "    axes[1, 0].axhline(1, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    # 5. Recommendations per item distribution\n",
    "    recs_per_item = [len(recs) for recs in item_recommendations.values()]\n",
    "    axes[1, 1].hist(\n",
    "        recs_per_item,\n",
    "        bins=min(20, max(recs_per_item) if recs_per_item else 1),\n",
    "        alpha=0.7,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "    axes[1, 1].set_xlabel(\"Number of Recommendations\")\n",
    "    axes[1, 1].set_ylabel(\"Number of Items\")\n",
    "    axes[1, 1].set_title(\"Distribution of Recommendations\\\\nper Item\")\n",
    "\n",
    "    # 6. Top items by recommendation count\n",
    "    item_rec_counts = {item: len(recs) for item, recs in item_recommendations.items()}\n",
    "    top_items = sorted(item_rec_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    if top_items:\n",
    "        items, counts = zip(*top_items, strict=False)\n",
    "        y_pos = np.arange(len(items))\n",
    "        axes[1, 2].barh(y_pos, counts)\n",
    "        axes[1, 2].set_yticks(y_pos)\n",
    "        axes[1, 2].set_yticklabels(\n",
    "            [item[:10] + \"...\" if len(item) > 10 else item for item in items],\n",
    "        )\n",
    "        axes[1, 2].set_xlabel(\"Number of Recommendations\")\n",
    "        axes[1, 2].set_title(\"Top 10 Items by\\\\nRecommendation Count\")\n",
    "        axes[1, 2].invert_yaxis()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No recommendations to visualize\")\n",
    "\n",
    "# Coverage analysis\n",
    "print(\"COVERAGE ANALYSIS:\")\n",
    "total_catalog_items = len(catalog_items)\n",
    "items_with_recs = len(item_recommendations)\n",
    "coverage_percentage = (\n",
    "    (items_with_recs / total_catalog_items * 100) if total_catalog_items > 0 else 0\n",
    ")\n",
    "\n",
    "print(f\"Total catalog items: {total_catalog_items}\")\n",
    "print(f\"Items with recommendations: {items_with_recs}\")\n",
    "print(f\"Coverage: {coverage_percentage:.1f}%\")\n",
    "\n",
    "# Identify items without recommendations (cold start problem)\n",
    "items_without_recs = set(catalog_items) - set(item_recommendations.keys())\n",
    "print(f\"   Items without recommendations: {len(items_without_recs)}\")\n",
    "\n",
    "if len(items_without_recs) > 0 and len(items_without_recs) <= 10:\n",
    "    print(f\"Items needing fallback: {list(items_without_recs)}\")\n",
    "elif len(items_without_recs) > 10:\n",
    "    print(f\"Sample items needing fallback: {list(items_without_recs)[:10]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fswe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
